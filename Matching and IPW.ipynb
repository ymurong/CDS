{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06c3216-756f-4a79-bd7f-cde152848b85",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Matching and IPW tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b07b7b-a9c2-4795-943e-dacd0d669cb8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e820c9e5-2a42-41ee-b2af-b5c3ddccbe8a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We define a simulator for vaccine hesitancy data, where all variables are binary. X represents vaccine hesitancy, T if an individual gets vaccinated and Y if they recover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dad8e83-191b-4372-94d5-8e610da2655d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vaccine_hesitancy_SCM(remove_confounding=False, n_samples=1000):\n",
    "    # X = vaccine hesitancy:\n",
    "    # epsilon_x\n",
    "    # epsilon_x = np.random.choice(2, n_samples, p = [0.7, 0.3], replace = True)\n",
    "    # Ber(0.3)\n",
    "    epsilon_x = np.random.binomial(n=1, p=0.3, size=n_samples)\n",
    "    # x: P(X=1);  1-x: P(X=0)\n",
    "    x = epsilon_x\n",
    "\n",
    "    # T = get vaccinated:\n",
    "\n",
    "    if remove_confounding:\n",
    "        # t = np.random.choice(2, n_samples, p=[0.5, 0.5], replace=True)\n",
    "        # Ber(0.5)\n",
    "        t = np.random.binomial(n=1, p=0.5, size=n_samples)\n",
    "    else:\n",
    "        # epsilon_t_0 = Ber(0.9):\n",
    "        # epsilon_t_0 = np.random.choice(2, n_samples, p=[0.1, 0.9], replace=True)\n",
    "        epsilon_t_0 = np.random.binomial(n=1, p=0.9, size=n_samples)\n",
    "        # epsilon_t_1 = Ber(0.1):\n",
    "        # epsilon_t_1 = np.random.choice(2, n_samples, p=[0.9, 0.1], replace=True)\n",
    "        epsilon_t_1 = np.random.binomial(n=1, p=0.1, size=n_samples)\n",
    "        # This is just a way to say that P(T|X=1) = P(epsilon_t_1) and P(T|X=0) = P(epsilon_t_0)\n",
    "        # P(T) = P(T|X=1)P(X=1) + P(T|X=0)P(X=0)\n",
    "        t = epsilon_t_1 * x + epsilon_t_0 * (1 - x)\n",
    "\n",
    "    # Y = recover\n",
    "    # epsilons for all combinations of values for X and T\n",
    "    # P(Y=1|X=0, T=0)= Ber(0.6)\n",
    "    epsilon_y_00 = np.random.choice(2, n_samples, p=[0.4, 0.6], replace=True)\n",
    "    # P(Y=1|X=0, T=1)= Ber(0.9)\n",
    "    epsilon_y_01 = np.random.choice(2, n_samples, p=[0.1, 0.9], replace=True)\n",
    "    # P(Y=1|X=1, T=0)= Ber(0.5)\n",
    "    epsilon_y_10 = np.random.choice(2, n_samples, p=[0.5, 0.5], replace=True)\n",
    "    # P(Y=1|X=1, T=1)= Ber(0.75)\n",
    "    epsilon_y_11 = np.random.choice(2, n_samples, p=[0.25, 0.75], replace=True)\n",
    "\n",
    "    # This is just a way to say that P(Y|X=0, T=0) = P(epsilon_y_00), P(Y|X=0, T=1) = P(epsilon_y_01) etc\n",
    "    # P(Y) = P(Y|X=1, T=1)P(X=1,T=1) + ...\n",
    "    y = x * t * epsilon_y_11 + (1 - x) * t * epsilon_y_01 + x * (1 - t) * epsilon_y_10 + (1 - x) * (\n",
    "            1 - t) * epsilon_y_00\n",
    "    df = pd.DataFrame({\"x\": x, \"t\": t, \"y\": y})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f261b5-b3ff-45fe-a9e5-2cae0b72e69e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We start by simulating some observational data for vaccine hesitancy, in which people who are not hesitant are also engaging in other behaviours that might help them recover. We compute the Average Treatment Effect (ATE) with these data in a naive way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efd4b62-e436-4614-85b7-d99c4610daa9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE in observational data: 0.3930467309263931\n"
     ]
    }
   ],
   "source": [
    "# Simulate some observational data:\n",
    "df = vaccine_hesitancy_SCM(n_samples=5000)\n",
    "\n",
    "# Vaccinated people:\n",
    "treatment_group = df[df[\"t\"] == 1]\n",
    "# Non-vaccinated people:\n",
    "control_group = df[df[\"t\"] == 0]\n",
    "\n",
    "# For X (our ajustment X), we make sure they have at least one observation for treatment and control groups\n",
    "if len(treatment_group[treatment_group[\"x\"] == 1]) == 0 or len(treatment_group[treatment_group[\"x\"] == 0]) == 0:\n",
    "    if len(control_group[control_group[\"x\"] == 1]) == 0 or len(control_group[control_group[\"x\"] == 0]) == 0:\n",
    "        print(\"Positivity is violated, simulate more samples.\")\n",
    "\n",
    "ATE_biased = np.mean(treatment_group[\"y\"]) - np.mean(control_group[\"y\"])\n",
    "\n",
    "print(\"ATE in observational data:\", ATE_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac82402-36e8-4447-bbe6-7a22668d334f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now simulate some data without any confounding, e.g. what we would get by running a randomized control trial and randomly assigning people to get vaccinated or not. We then compute the Average Treatment Effect (ATE) with these data, which is an unbiased estimate of the real ATE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fba5ff-b588-4a3f-96fe-32abff18d9fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE in an uncofounded setting (e.g. randomized controlled trial (RCT)): 0.2818701816105551 (should be close to 0.3)\n"
     ]
    }
   ],
   "source": [
    "df_rct = vaccine_hesitancy_SCM(n_samples=10000, remove_confounding=True)\n",
    "# Vaccinated people:\n",
    "treatment_group_rct = df_rct[df_rct[\"t\"] == 1]\n",
    "# Non-vaccinated people:\n",
    "control_group_rct = df_rct[df_rct[\"t\"] == 0]\n",
    "\n",
    "ATE_rct = np.mean(treatment_group_rct[\"y\"]) - np.mean(control_group_rct[\"y\"])\n",
    "\n",
    "print(\"ATE in an uncofounded setting (e.g. randomized controlled trial (RCT)):\", ATE_rct, \"(should be close to 0.3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb8c83-bb99-411e-976d-de5fa92ff8ca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that the estimates for the ATEs are fairly different in the two settings (and we know the ATE in the RCT is supposed to be an approximation of the true ATE, which is close to 0.3), and that the ATE without correction overestimates the effect of the vaccine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90505e-f8f4-4e0d-a414-36e730df4090",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exact matching:\n",
    "\n",
    "We can now conceptually pair each individual in the treatment group with $X=x$ with an individual in the control group with $X=x$, and remove the ones which don't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cfc792-0ee1-4aee-a05f-68197707a8c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with X=0 in treatment:  3123  and in control:  350\n",
      "Number of people with X=1 in treatment:  144  and in control:  1383\n"
     ]
    }
   ],
   "source": [
    "treatment_group_x_0 = treatment_group[treatment_group[\"x\"] == 0]\n",
    "treatment_group_x_1 = treatment_group[treatment_group[\"x\"] == 1]\n",
    "\n",
    "control_group_x_0 = control_group[control_group[\"x\"] == 0]\n",
    "control_group_x_1 = control_group[control_group[\"x\"] == 1]\n",
    "\n",
    "print(\"Number of people with X=0 in treatment: \", len(treatment_group_x_0), \" and in control: \", len(control_group_x_0))\n",
    "print(\"Number of people with X=1 in treatment: \", len(treatment_group_x_1), \" and in control: \", len(control_group_x_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3cb152-a0df-47f0-a56d-844bb6437175",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The numbers of people with X=0 and X=1 is quite unbalanced in the control and treatment.\n",
    "We can also check covariate balancing on the original data P(X|T=0) should the same as P(X|T=1), or in finite data relatively close (we can use standardized mean difference to check how close they are in distribution). Since X is a binary variable, we can just check what happens for X=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81639537-4a56-4c32-9925-c797f8462282",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of X=0 on total in treatment:  0.9559228650137741  and in control:  0.20196191575302944\n"
     ]
    }
   ],
   "source": [
    "# approximating P(X=0|T=1)\n",
    "freq_x0_treatment = len(treatment_group_x_0) / len(treatment_group)\n",
    "# approximating P(X=0|T=0)\n",
    "freq_x0_control = len(control_group_x_0) / len(control_group)\n",
    "# In order to be balanced these two should be very close :\n",
    "print(\"Proportion of X=0 on total in treatment: \", freq_x0_treatment, \" and in control: \", freq_x0_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb16df-7f11-498c-8c85-f2ef86a14ee8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now perform the matching. Since all units with X=x are conceptually the same, we don't need to actually match the pairs, but we can just keep the same number of people for treatment and control groups for each value of X=x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426c2716-04f7-4fa2-a5be-e3a66884cc54",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: number of people with X=0 in treatment:  350  and in control:  350\n",
      "After balancing: number of people with X=1 in treatment:  144  and in control:  144\n"
     ]
    }
   ],
   "source": [
    "min_number_x0 = min(len(treatment_group_x_0), len(control_group_x_0))\n",
    "balanced_treatment_x_0 = treatment_group_x_0[0:min_number_x0]\n",
    "balanced_control_x_0 = control_group_x_0[0:min_number_x0]\n",
    "print(\"After balancing: number of people with X=0 in treatment: \", len(balanced_treatment_x_0), \" and in control: \",\n",
    "      len(balanced_control_x_0))\n",
    "\n",
    "min_number_x1 = min(len(treatment_group_x_1), len(control_group_x_1))\n",
    "balanced_treatment_x_1 = treatment_group_x_1[0:min_number_x1]\n",
    "balanced_control_x_1 = control_group_x_1[0:min_number_x1]\n",
    "print(\"After balancing: number of people with X=1 in treatment: \", len(balanced_treatment_x_1), \" and in control: \",\n",
    "      len(balanced_control_x_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c5031-6b66-4ebb-ae25-b3dc8ec3cb2d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now check again the covariate balancing and see that they are balanced and then just use this smaller dataset to estimate the ATE (which turns out to be closer to the true value than the original confounded case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d08091-e58d-4f73-8ea8-cf5a38e674b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of X=0 on total in treatment:  0.708502024291498  and in control:  0.708502024291498\n",
      "ATE after balancing: 0.2935222672064778\n"
     ]
    }
   ],
   "source": [
    "balanced_treatment = pd.concat([balanced_treatment_x_0, balanced_treatment_x_1])\n",
    "balanced_control = pd.concat([balanced_control_x_1, balanced_control_x_0])\n",
    "\n",
    "# approximating P(X=0|T=1)\n",
    "freq_x0_treatment_matching = len(balanced_treatment_x_0) / len(balanced_treatment)\n",
    "# approximating P(X=0|T=0)\n",
    "freq_x0_control_matching = len(balanced_control_x_0) / len(balanced_control)\n",
    "print(\"Proportion of X=0 on total in treatment: \", freq_x0_treatment_matching, \" and in control: \",\n",
    "      freq_x0_control_matching)\n",
    "\n",
    "ATE_matching = np.mean(balanced_treatment[\"y\"]) - np.mean(balanced_control[\"y\"])\n",
    "print(\"ATE after balancing:\", ATE_matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812a878-4688-49bb-bda1-09f1c74aff91",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now used only one variable X for simplicity as the adjustment set. In general this method works also with more than one variable in the adjustment set, and if the variables are not discrete can also use a distance metric to match the units. Unfortunately, this method discared a lot of data, making the estimation less stable.\n",
    "\n",
    "## Propensity scores:\n",
    "We now compute $P(T=1|X)$ for each value of $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d1dd9c-d2a6-43f1-bb81-477fb8bbcbc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8992225741433919 : true value 0.9,  0.09430255402750491 : true value 0.1\n"
     ]
    }
   ],
   "source": [
    "x0_group = pd.concat([treatment_group_x_0, control_group_x_0])\n",
    "x1_group = pd.concat([treatment_group_x_1, control_group_x_1])\n",
    "\n",
    "propensity_score_x0 = len(treatment_group_x_0) / len(x0_group)\n",
    "propensity_score_x1 = len(treatment_group_x_1) / len(x1_group)\n",
    "\n",
    "print(propensity_score_x0, \": true value 0.9, \", propensity_score_x1, \": true value 0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8558d56-7067-443f-890b-a9bfb3f6b3f2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We could do matching on the values of propensity scores (this is called propensity score matching), but in this case it doesn't really improve things since the probabilities are quite different for every combination of X. Otherwise it can be seen as a way of clustering values of X that have a similar effect on T.\n",
    "\n",
    "## Inverse probability weighting\n",
    "\n",
    "Instead we just use the formula to compute a weighted average for treatment and control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433b3e9c-09e3-4c1e-b2fc-308c3a0ad267",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE after IPW: 0.30833587378170557\n"
     ]
    }
   ],
   "source": [
    "number_of_all_samples = len(df)\n",
    "\n",
    "weighted_sum_treatment_0 = sum(treatment_group_x_0[\"y\"]) / propensity_score_x0\n",
    "weighted_sum_treatment_1 = sum(treatment_group_x_1[\"y\"]) / propensity_score_x1\n",
    "\n",
    "mean_treatment = (weighted_sum_treatment_0 + weighted_sum_treatment_1) / number_of_all_samples\n",
    "\n",
    "weighted_sum_control_0 = sum(control_group_x_0[\"y\"]) / (1 - propensity_score_x0)\n",
    "weighted_sum_control_1 = sum(control_group_x_1[\"y\"]) / (1 - propensity_score_x1)\n",
    "\n",
    "mean_control = (weighted_sum_control_0 + weighted_sum_control_1) / number_of_all_samples\n",
    "\n",
    "ATE_IPW = mean_treatment - mean_control\n",
    "print(\"ATE after IPW:\", ATE_IPW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c8fef-d6d4-4a95-9dd1-461c7b70eee2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing the mean and variance of the estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0145dbe3-3d19-4f20-96e0-d8bb51d1d9b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3103716950094477 0.03829507656126163 0.2795275590551181 5.551115123125783e-17 0.28314606352325483 0.013109247175140108\n"
     ]
    }
   ],
   "source": [
    "def IPW_XTY(df):\n",
    "    # Vaccinated people:\n",
    "    treatment_group = df[df[\"t\"] == 1]\n",
    "    # Non-vaccinated people:\n",
    "    control_group = df[df[\"t\"] == 0]\n",
    "\n",
    "    number_of_all_samples = len(df)\n",
    "    treatment_group_x_0 = treatment_group[treatment_group[\"x\"] == 0]\n",
    "    treatment_group_x_1 = treatment_group[treatment_group[\"x\"] == 1]\n",
    "\n",
    "    control_group_x_0 = control_group[control_group[\"x\"] == 0]\n",
    "    control_group_x_1 = control_group[control_group[\"x\"] == 1]\n",
    "\n",
    "    weighted_sum_treatment_0 = sum(treatment_group_x_0[\"y\"]) / propensity_score_x0\n",
    "    weighted_sum_treatment_1 = sum(treatment_group_x_1[\"y\"]) / propensity_score_x1\n",
    "\n",
    "    mean_treatment = weighted_sum_treatment_0 + weighted_sum_treatment_1\n",
    "    mean_treatment = mean_treatment / number_of_all_samples\n",
    "\n",
    "    weighted_sum_control_0 = sum(control_group_x_0[\"y\"]) / (1 - propensity_score_x0)\n",
    "    weighted_sum_control_1 = sum(control_group_x_1[\"y\"]) / (1 - propensity_score_x1)\n",
    "\n",
    "    mean_control = weighted_sum_control_0 + weighted_sum_control_1\n",
    "    mean_control = mean_control / number_of_all_samples\n",
    "\n",
    "    return mean_treatment - mean_control\n",
    "\n",
    "\n",
    "def exactMatching_XTY(df):\n",
    "    # Vaccinated people:\n",
    "    treatment_group = df[df[\"t\"] == 1]\n",
    "    # Non-vaccinated people:\n",
    "    control_group = df[df[\"t\"] == 0]\n",
    "\n",
    "    treatment_group_x_0 = treatment_group[treatment_group[\"x\"] == 0]\n",
    "    treatment_group_x_1 = treatment_group[treatment_group[\"x\"] == 1]\n",
    "\n",
    "    control_group_x_0 = control_group[control_group[\"x\"] == 0]\n",
    "    control_group_x_1 = control_group[control_group[\"x\"] == 1]\n",
    "\n",
    "    min_number_x0 = min(len(treatment_group_x_0), len(control_group_x_0))\n",
    "    balanced_treatment_x_0 = treatment_group_x_0[0:min_number_x0]\n",
    "    balanced_control_x_0 = control_group_x_0[0:min_number_x0]\n",
    "\n",
    "    min_number_x1 = min(len(treatment_group_x_1), len(control_group_x_1))\n",
    "    balanced_treatment_x_1 = treatment_group_x_1[0:min_number_x1]\n",
    "    balanced_control_x_1 = control_group_x_1[0:min_number_x1]\n",
    "\n",
    "    return np.mean(balanced_treatment[\"y\"]) - np.mean(balanced_control[\"y\"])\n",
    "\n",
    "\n",
    "ATE_IPW_list = []\n",
    "ATE_matching_list = []\n",
    "ATE_rct_list = []\n",
    "\n",
    "for i in range(0, 100):\n",
    "    df = vaccine_hesitancy_SCM(n_samples=5000)\n",
    "    ATE_IPW_list.append(IPW_XTY(df))\n",
    "    ATE_matching_list.append(exactMatching_XTY(df))\n",
    "    df_rct = vaccine_hesitancy_SCM(n_samples=5000, remove_confounding=True)\n",
    "    # Vaccinated people:\n",
    "    treatment_group_rct = df_rct[df_rct[\"t\"] == 1]\n",
    "    # Non-vaccinated people:\n",
    "    control_group_rct = df_rct[df_rct[\"t\"] == 0]\n",
    "    ATE_rct_list.append(np.mean(treatment_group_rct[\"y\"]) - np.mean(control_group_rct[\"y\"]))\n",
    "\n",
    "print(np.mean(ATE_IPW_list), np.std(ATE_IPW_list), np.mean(ATE_matching_list), np.std(ATE_matching_list),\n",
    "      np.mean(ATE_rct_list), np.std(ATE_rct_list))\n",
    "\n",
    "### what is the conclusion???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545226ae-4e20-495e-8b26-0357c7554e5b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Example with more variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96ce6339-3d7d-4f00-8565-1eb17032fcf4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE in an uncofounded setting (e.g. randomized controlled trial (RCT)): 0.3093618044307901 , unadjusted ATE: 0.30018967582270206\n",
      "After balancing: number of people with X1=0 and X2=0 in treatment:  215  and in control:  215\n",
      "After balancing: number of people with X1=0 and X2=1 in treatment:  237  and in control:  237\n",
      "After balancing: number of people with X1=1 and X2=0 in treatment:  267  and in control:  267\n",
      "After balancing: number of people with X1=1 and X2=1 in treatment:  30  and in control:  30\n"
     ]
    }
   ],
   "source": [
    "def more_variables_SCM(remove_confounding=False, n_samples=1000):\n",
    "    epsilon_x1 = np.random.choice(2, n_samples, p=[0.6, 0.4], replace=True)\n",
    "    x1 = epsilon_x1\n",
    "\n",
    "    epsilon_x2 = np.random.choice(2, n_samples, p=[0.7, 0.3], replace=True)\n",
    "    x2 = epsilon_x2\n",
    "\n",
    "    if remove_confounding:\n",
    "        t = np.random.choice(2, n_samples, p=[0.5, 0.5], replace=True)\n",
    "    else:\n",
    "        epsilon_t_00 = np.random.choice(2, n_samples, p=[0.1, 0.9], replace=True)\n",
    "        epsilon_t_10 = np.random.choice(2, n_samples, p=[0.8, 0.2], replace=True)\n",
    "        epsilon_t_11 = np.random.choice(2, n_samples, p=[0.95, 0.05], replace=True)\n",
    "        epsilon_t_01 = np.random.choice(2, n_samples, p=[0.7, 0.3], replace=True)\n",
    "        t = epsilon_t_00 * (1 - x1) * (1 - x2) + epsilon_t_01 * (1 - x1) * x2 + epsilon_t_10 * x1 * (\n",
    "                1 - x2) + epsilon_t_11 * x1 * x2\n",
    "\n",
    "    x = x1 * x2\n",
    "    epsilon_y_00 = np.random.choice(2, n_samples, p=[0.4, 0.6], replace=True)\n",
    "    epsilon_y_01 = np.random.choice(2, n_samples, p=[0.1, 0.9], replace=True)\n",
    "    epsilon_y_10 = np.random.choice(2, n_samples, p=[0.5, 0.5], replace=True)\n",
    "    epsilon_y_11 = np.random.choice(2, n_samples, p=[0.25, 0.75], replace=True)\n",
    "    y = x * t * epsilon_y_11 + (1 - x) * t * epsilon_y_01 + x * (1 - t) * epsilon_y_10 + (1 - x) * (\n",
    "            1 - t) * epsilon_y_00\n",
    "\n",
    "    df = pd.DataFrame({\"x1\": x1, \"x2\": x2, \"t\": t, \"y\": y})\n",
    "    return df\n",
    "\n",
    "\n",
    "# Simulate some observational data:\n",
    "df2 = more_variables_SCM(n_samples=5000)\n",
    "\n",
    "# Vaccinated people:\n",
    "t_group = df2[df2[\"t\"] == 1]\n",
    "# Non-vaccinated people:\n",
    "c_group = df2[df2[\"t\"] == 0]\n",
    "\n",
    "ATE_biased = np.mean(t_group[\"y\"]) - np.mean(c_group[\"y\"])\n",
    "df2_rct = more_variables_SCM(n_samples=10000, remove_confounding=True)\n",
    "# Vaccinated people:\n",
    "t_rct = df2_rct[df2_rct[\"t\"] == 1]\n",
    "# Non-vaccinated people:\n",
    "c_rct = df2_rct[df2_rct[\"t\"] == 0]\n",
    "print(\"ATE in an uncofounded setting (e.g. randomized controlled trial (RCT)):\",\n",
    "      np.mean(t_rct[\"y\"]) - np.mean(c_rct[\"y\"]), \", unadjusted ATE:\", ATE_biased)\n",
    "\n",
    "treatment_group_x_0 = t_group[t_group[\"x1\"] == 0]\n",
    "treatment_group_x_1 = t_group[t_group[\"x1\"] == 1]\n",
    "\n",
    "treatment_group_x_00 = treatment_group_x_0[treatment_group_x_0[\"x2\"] == 0]\n",
    "treatment_group_x_10 = treatment_group_x_1[treatment_group_x_1[\"x2\"] == 0]\n",
    "treatment_group_x_01 = treatment_group_x_0[treatment_group_x_0[\"x2\"] == 1]\n",
    "treatment_group_x_11 = treatment_group_x_1[treatment_group_x_1[\"x2\"] == 1]\n",
    "\n",
    "control_group_x_0 = c_group[c_group[\"x1\"] == 0]\n",
    "control_group_x_1 = c_group[c_group[\"x1\"] == 1]\n",
    "\n",
    "control_group_x_00 = control_group_x_0[control_group_x_0[\"x2\"] == 0]\n",
    "control_group_x_10 = control_group_x_1[control_group_x_1[\"x2\"] == 0]\n",
    "control_group_x_01 = control_group_x_0[control_group_x_0[\"x2\"] == 1]\n",
    "control_group_x_11 = control_group_x_1[control_group_x_1[\"x2\"] == 1]\n",
    "\n",
    "min_number_x_00 = min(len(treatment_group_x_00), len(control_group_x_00))\n",
    "balanced_treatment_x_00 = treatment_group_x_00[0:min_number_x_00]\n",
    "balanced_control_x_00 = control_group_x_00[0:min_number_x_00]\n",
    "print(\"After balancing: number of people with X1=0 and X2=0 in treatment: \", len(balanced_treatment_x_00),\n",
    "      \" and in control: \", len(balanced_control_x_00))\n",
    "\n",
    "min_number_x_01 = min(len(treatment_group_x_01), len(control_group_x_01))\n",
    "balanced_treatment_x_01 = treatment_group_x_01[0:min_number_x_01]\n",
    "balanced_control_x_01 = control_group_x_01[0:min_number_x_01]\n",
    "print(\"After balancing: number of people with X1=0 and X2=1 in treatment: \", len(balanced_treatment_x_01),\n",
    "      \" and in control: \", len(balanced_control_x_01))\n",
    "\n",
    "min_number_x_10 = min(len(treatment_group_x_10), len(control_group_x_10))\n",
    "balanced_treatment_x_10 = treatment_group_x_10[0:min_number_x_10]\n",
    "balanced_control_x_10 = control_group_x_10[0:min_number_x_10]\n",
    "print(\"After balancing: number of people with X1=1 and X2=0 in treatment: \", len(balanced_treatment_x_10),\n",
    "      \" and in control: \", len(balanced_control_x_10))\n",
    "\n",
    "min_number_x_11 = min(len(treatment_group_x_11), len(control_group_x_11))\n",
    "balanced_treatment_x_11 = treatment_group_x_11[0:min_number_x_11]\n",
    "balanced_control_x_11 = control_group_x_11[0:min_number_x_11]\n",
    "print(\"After balancing: number of people with X1=1 and X2=1 in treatment: \", len(balanced_treatment_x_11),\n",
    "      \" and in control: \", len(balanced_control_x_11))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15b574f5-b4ae-472a-b899-7d50e9c26e56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE after balancing: 0.28971962616822433\n"
     ]
    }
   ],
   "source": [
    "balanced_treatment2 = pd.concat(\n",
    "    [balanced_treatment_x_00, balanced_treatment_x_01, balanced_treatment_x_10, balanced_treatment_x_11])\n",
    "balanced_control2 = pd.concat(\n",
    "    [balanced_control_x_01, balanced_control_x_00, balanced_control_x_11, balanced_control_x_10])\n",
    "\n",
    "ATE_matching2 = np.mean(balanced_treatment2[\"y\"]) - np.mean(balanced_control2[\"y\"])\n",
    "print(\"ATE after balancing:\", ATE_matching2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d0953-deb6-4b07-ab1b-d17d4c552da8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Exact matching discarded a lot of data, so the estimate is quite unstable. We now look quickly at propensity scores, which are again not very useful in this case, and then we look at inverse probability weighting which does not discard any data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd5950-82ad-43cf-a5a2-0c741926732f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Propensity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcc26c-b54d-4cb6-86b4-61256f2e2f76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now compute $P(T=1|X)$ for each value of $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ced8671-b333-48c0-9499-e38201009e61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x00_group = pd.concat([treatment_group_x_00, control_group_x_00])\n",
    "x10_group = pd.concat([treatment_group_x_10, control_group_x_10])\n",
    "x01_group = pd.concat([treatment_group_x_01, control_group_x_01])\n",
    "x11_group = pd.concat([treatment_group_x_11, control_group_x_11])\n",
    "\n",
    "propensity_score_x00 = len(treatment_group_x_00) / len(x00_group)\n",
    "propensity_score_x10 = len(treatment_group_x_10) / len(x10_group)\n",
    "propensity_score_x01 = len(treatment_group_x_01) / len(x01_group)\n",
    "propensity_score_x11 = len(treatment_group_x_11) / len(x11_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b986a-4a1f-494b-bd6f-71ab32550538",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We could do matching on the values of propensity scores (this is called propensity score matching), but in this case it doesn't really improve things since the probabilities are quite different for every combination of X. Otherwise it can be seen as a way of clustering values of X that have a similar effect on T."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79104ca-d38d-4d53-b4a8-ce26be5da08b",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inverse probability weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d43da6b5-bc9d-46a5-8bad-581b0eba2d81",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE after IPW: 0.3090427537425747\n"
     ]
    }
   ],
   "source": [
    "number_of_all_samples = len(df2)\n",
    "\n",
    "weighted_sum_treatment_00 = sum(treatment_group_x_00[\"y\"]) / propensity_score_x00\n",
    "weighted_sum_treatment_01 = sum(treatment_group_x_01[\"y\"]) / propensity_score_x01\n",
    "weighted_sum_treatment_10 = sum(treatment_group_x_10[\"y\"]) / propensity_score_x10\n",
    "weighted_sum_treatment_11 = sum(treatment_group_x_11[\"y\"]) / propensity_score_x11\n",
    "\n",
    "mean_treatment2 = weighted_sum_treatment_00 + weighted_sum_treatment_01 + weighted_sum_treatment_10 + weighted_sum_treatment_11\n",
    "mean_treatment2 = mean_treatment2 / number_of_all_samples\n",
    "\n",
    "weighted_sum_control_00 = sum(control_group_x_00[\"y\"]) / (1 - propensity_score_x00)\n",
    "weighted_sum_control_01 = sum(control_group_x_01[\"y\"]) / (1 - propensity_score_x01)\n",
    "weighted_sum_control_10 = sum(control_group_x_10[\"y\"]) / (1 - propensity_score_x10)\n",
    "weighted_sum_control_11 = sum(control_group_x_11[\"y\"]) / (1 - propensity_score_x11)\n",
    "\n",
    "mean_control2 = weighted_sum_control_00 + weighted_sum_control_01 + weighted_sum_control_10 + weighted_sum_control_11\n",
    "mean_control2 = mean_control2 / number_of_all_samples\n",
    "\n",
    "ATE_IPW2 = mean_treatment2 - mean_control2\n",
    "print(\"ATE after IPW:\", ATE_IPW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10582e-4921-4859-98d1-a3843a063305",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}